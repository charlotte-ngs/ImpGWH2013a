---
title: "Literate Programming Based Improvements To Chapter 1 of GWH2013a"
author: "Peter von Rohr"
date: "29 Jan 2015"
layout: post
output:
  html_document:
    keep_md: no
    theme: united
    mathjax: "http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
references:
- URL: http://www.springer.com/life+sciences/systems+biology+and+bioinformatics/book/978-1-62703-446-3
  author:
  - family: Gondro
    given: Cedric
  - family: van der Werf
    given: Julius
  - family: Hayes
    given: Ben
  id: GWH2013a
  issued:
    year: 2013
  title: Genome-Wide Association Studies and Genomic Prediction
---

```{r, echo=FALSE}
### # fix rng seed
set.seed(1234)
### # options whether to re-generate data initially and whether to keep data at the end
bReGenerateData <- TRUE
bKeepData <- FALSE

### # define file names of data files
sDataDn <- "../data"
sSnpPhenFn <- file.path(sDataDn, "square.txt")
sSnpFn <- file.path(sDataDn, "genotypes.txt")

### # dimensionality of SNP matrix
nNrSnp <- 50000
nNrSamples <- 1000

### # data simulation, as given in R-scripts downloaded from Springer
if (bReGenerateData) {
  matSnp <- matrix(sample(c(0,1,2),nNrSnp*nNrSamples,replace=T),nNrSnp,nNrSamples)
  dfSnp <- data.frame(matSnp)
  names(dfSnp) <- paste("ID",1:nNrSamples,sep="")
  rownames(dfSnp) <- paste("SNP",1:nNrSnp,sep="")
  dfSnp <- rbind(sample(c("A","N"),nNrSamples,replace=T),rnorm(nNrSamples),dfSnp)
  rownames(dfSnp)[1:2] <- c("status","phenotype")
  
  ### # check whether data dir exists
  if (! file.exists(sDataDn)) dir.create(sDataDn)
  ### # write snp and phenotypes
  write.table(t(dfSnp), file = sSnpPhenFn, quote = F, col.names = T, row.names = T, sep = "\t")
  ### # genotypes only  
  write.table(dfSnp[-c(1,2),], file = sSnpFn, quote = F, col.names = F, row.names = F, sep = "\t")
  
}


```

## Disclaimer
This document summarizes the content of chapter 1 of [@GWH2013a]. The main objective of this summary is to transform the summarized text and the programming code between the text into a document based on the paradigm of [`Literate Programming`](http://en.wikipedia.org/wiki/Literate_programming). 

## Chapter 1 - R for Genome Wide Association Studies

### Abstract
The authors state that R has become the de facto language of choice for statisticians and that it is the most widely used environment for analysing high-throughput genomic data. While these statements are certainly not wrong, one has to keep in mind that most of statistical analyses are probably still done in MS Excel. But despite that fact I would never recommend to anyone to use MS Excel for any statistical analysis. Or more generally I would not make any recommendation of a product only based on the fact that a large number of people use that product. From my point of view, R is widely used because it has a very active and supportive community and that problems are solved much quicker than in any other commericially available products. 


## Section 1 - Introduction
The introduction starts with the same statements as were already mentioned at the begining of the abstract. Do not get me wrong here, I am not critisizing the authors here for saying anything wrong, but I find those arguments not so convincing. Part of the problem is also that no examples of R-code snippets are shown to support their argument. 

Let me add two points in favor of R that I would use to convince people to use R. For all those readers who are not interested in these arguments or for all those who are already convinced of R, can safely skip the remainder of this subsection and can jump to the summary of section 2 of chapter 1 of [@GWH2013a].

My favorite two arguments in favor of R are

1. `Fast Prototyping` - which means that ideas are quickly converted into R-code and
2. `Literate Programming` - R provides a lot of infrastructure to implement the paradigm of literate programming. 

For all those who want to know more about the above two points, please read the post entitled [Why R](http://charlotte-ngs.github.io/WhyR/notes/20150203-WhyR.html). 

After this short digression, let us get back to [@GWH2013a]


## Section 2 - Reading Data into R
It is true that it takes time to read genomic data of a certain size into R, but this is true for any system. Using the same dataset as in [@GWH2013a], the following timings result

```{r}
sSnpDataFn <- "../data/square.txt"
(ptReadTable  <- system.time(myData <- read.table(file = sSnpDataFn, sep = "\t")))
dim(myData)
```

### What about MS Excel?
As a very small side note, when looking at the dimension of the data.frame `myData`, we can see that it has `r ncol(myData)` columns. According to [Excel specifications and limits](https://support.office.microsoft.com/en-us/article/Excel-specifications-and-limits-ca36e2dc-1f09-4620-b726-67c00b05040f?CTT=1&CorrelationId=f389929d-fd39-4369-b67d-a8061edd2258&ui=en-US&rs=en-US&ad=US) the number of columns that can be read into a single worksheet in Excel is still limited to 16,384 which makes it clearly impossible to read this dataset into a single worksheet of MS Excel. I clearly prefer waiting for `r as.numeric(ptReadTable[3])` seconds until the data is read compared to not being able to read the data at all. 

Back on track to where we were before that side note, we still want to read that SNP dataset into R and we would like to do it a little faster then what we just saw with a bare-bones `read.table` command. One way of doing this is by telling the `read.table()` function what datatype to expect in which column. Datatypes are specified using the argument `colClasses` which is set to a vector of strings where each vector element contains the datatype of the corresponding datacolumn. 

```{r}
(ptReadTableColClasses <- system.time(myData <- read.table(file = sSnpDataFn, sep = "\t", 
                                                           colClasses = c("character", "factor", 
                                                                          rep("numeric",50001)))))
```

### Uups why is this not working?
When we compare the above statement to the original one in [@GWH2013a], we can see that there is a difference. The vector assigned to the `colClasses` argument in the book is missing the first element. The statement in the book when copy-pasted to the R-prompt gives the following error:

```
myData <- read.table(file = sSnpDataFn, sep = "\t", colClasses = c("factor", rep("numeric",50001)))
Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  : 
  scan() expected 'a real', got 'A'
```

The reason for the error can be found in the help file to `?read.table()` which says for the argument colClasses that row names must also be specified in the vector of column classes. Therefore when adding "character" as the first element of the colClasses vector, the error disappears. 

### Bummer - Literate Programming Rocks!
This is really unbelievable, the authors of the first chapter in [@GWH2013a] could not have given me a bigger gift with this error. I really feel blessed to be able to present such a fantastic show-case in favor of literate programming. Would the authors have used a literate programming approach, such as we are doing here, they could have avoided for themselves the embarrasing and painful experience of showing an incorrect statements already in the second code snippet that they are presenting. For the reader of the book who really wants to use the examples, this is very frustrating when you hit an error already in the second statement that could be found. 

In the text after the code snippet producing the error, there is yet another problem. The authors mention that performance could still be marginally improved by specifying the number of rows in the table beforehand using in out case `ncols=50000`. But this does not work. First, we do not have 50000 rows but only `r nrow(myData)` rows. Second, the function `read.table()` does not have an argument called `ncol`. Such an argument would make no sense at all, because the number of columns is already determined by the colClasses vector. Probably, what the authors wanted to say is to specify the number of rows using the `nrows` argument such as shown below. 

Adding more options concerning the comment character and the number of rows, we can hope to still get a little faster.

```{r}
(ptReadTableNRows <- system.time(myData <- read.table(file = sSnpDataFn, sep = "\t", 
                                                           colClasses = c("character", "factor", 
                                                                          rep("numeric",50001)),
                                                           comment.char = "",
                                                           nrows = 1000)))

```

### Comparing read.table results
So far we have seen three different versions of how to read a dataset into R using the `read.table()` function. Now let us compare the timings that we have measured so far.


```{r, echo=FALSE}
nImpColClasses <- round((as.numeric(ptReadTable[3])-as.numeric(ptReadTableColClasses[3]))/
                          as.numeric(ptReadTable[3]), 
                        digits = 2)
nImpNRows <- round((as.numeric(ptReadTable[3])-as.numeric(ptReadTableNRows[3]))/
                          as.numeric(ptReadTable[3]), 
                        digits = 2)
```

Read.table version | Total time (in s)                        | Improvement over minimal (in %)
------------------ | ---------------------------------------- | -------------------------------
minimal            | `r as.numeric(ptReadTable[3])`           | 
colClasses         | `r as.numeric(ptReadTableColClasses[3])` | `r 100 * nImpColClasses`
nRows              | `r as.numeric(ptReadTableNRows[3])`      | `r 100 * nImpNRows`

Comparing the timings in the above table, we observe much smaller differences than reported in [@GWH2013a]. It is difficult to find a reason for that because such timings are very difficult to compare across machines and also between different versions of R. Useful information on telling the reader how results were produced is included in the section `Session Info`. The content of this section is just the output of the R function `sessionInfo()`. 

### Alternatives to read.table
Using the function `read.table()` we were able to read all data, phenotypes and genotypes from the same file. In principle we did not even have to know the dimension of the dataset or what datatypes were contained in which columns, `read.table()` would figure it out all by itself. 

Now let us assume that genotypes are stored in a separate file and that they are all encoded by numerical values. Furthermore, we know the dimensions of the genoptypes matrix corresponding to the number of SNPs times the number of samples. Then it is possible to read the genotypes using the function `scan()`. Because `scan()` is a low-level `C`-function, it should be faster than `read.table()` and combining `scan()` together with `matrix()` is probably the fastest way to read data of tabular form into R.

```{r}
(ptMatrixScan <- system.time(myGeno <- matrix(scan(file = "../data/genotypes.txt", 
                                                   what = integer(), 
                                                   sep = "\t"), 
                                              nrow = 50000, 
                                              ncol = 1000, 
                                              byrow = T)))
```

This reads all genotypes from a file and stores it in a matrix of dimension `r nrow(myGeno)` $\times$ `r ncol(myGeno)`. Reading the genotypes with `scan()` takes `r as.numeric(ptMatrixScan[3])` seconds. Adding this result to the above table allows us to compare all methods that transform the input SNP genotypes into a tabular form. The comparison is not completely fair, because `read.table()` also read the phenotypes which is not done by `matrix(scan())`. 

```{r, echo=FALSE}
nImpMatScan <- round((as.numeric(ptReadTable[3])-as.numeric(ptMatrixScan[3]))/as.numeric(ptReadTable[3]), digits = 2)
```

Input version      | Total time (in s)                        | Improvement over minimal (in %)
------------------ | ---------------------------------------- | -------------------------------
minimal            | `r as.numeric(ptReadTable[3])`           | 
colClasses         | `r as.numeric(ptReadTableColClasses[3])` | `r 100 * nImpColClasses`
nRows              | `r as.numeric(ptReadTableNRows[3])`      | `r 100 * nImpNRows`
matrix-scan        | `r as.numeric(ptMatrixScan[3])`          | `r 100 * nImpMatScan`

When looking at the above table it is clear that only `matrix(scan())` leads to a significant improvement of the time it takes to import our example dataset. But specifying all the options like number of rows or column classes has an other important advantage. It allows us to check the consistency between what we think should be in the dataset and what R really finds in data file specified as input.  

In case, we do not need the data in a tabular form, then we can use functions like `readLines()` or `readChar()`. The former reads the content into a vector of characters. Each line of the input file is stored as one vector component. For `readChar()` a character vector of length corresponding to the number of items read is returned. These functions `readLines()` and `readChar()` are faster than `scan()` but they are only useful, if we can use the data as vector of characters. First reading the data using `readLines()` or `readChar()` and then parsing the data using some high-level R-functions is probably not faster than using `scan` in the first place. Because the result that we get from `read.table()` and `matrix(scan())` on the one side and from `readLines()` and `readChar()` on the other side is very different, a direct comparison of the timing results does not make a whole lot of sense. 

The function `readLines()` can be very useful if we have a really big dataset that does not fit into memory, then we can use the following construct which reads the content line by line and stores the current line in `curLine`. 

```{r}
conInput <- file(description = "../data/genotypes.txt", open = "r")
nrLine <- 0
sumFirstElement <- 0
while (length(curLine  <- readLines(con = conInput, n = 1)) > 0) {
  vecCurElement <- unlist(strsplit(curLine, split = "\t", perl = TRUE))
  sumFirstElement <- sumFirstElement + as.numeric(vecCurElement[1])
  nrLine <- nrLine + 1
}
close(conInput)
cat("Number of lines read: ", nrLine, "\n")
cat("Sum over first elements: ", sumFirstElement)
```

The above code snippet is not doing anything terribly clever, but it just illustrates how one can use `readLines()` for reading a file line-by-line. Once a line is read from the file, it is stored in the one element character vector `curLine` which is then parsed into its elements in splitting the line at the separator. After the split, the above code just sums over the first element of each line and it counts the number of lines read.

With the advent of big data, the above construct can become more and more important, because nowadays, we often do not have the complete dataset to be analysed in one single file. Most likely we just have a stream of data that is sent to us from some server and we just have to process that stream. For example, suppose we want to analyse certain properties of SNPs from a large SNP-database or we want to link certain characteristics of SNPs to properties that are found in sequence databases such as Genebank, but we do not have neither the time to wait for the SNP-database and the complete Genebank database to be downloaded nor do we have the money to store all this data. So what we try to do is to send requests to these database servers to send us a stream of interesting data and we are processing this data on the fly and storing just the results. 


### Conclusions of Section 2
As we have shown with our timings in the section above, reading data into R does require some time. But still, for reading a dataset in the order of $10^7$ data points requires less than one minute to load. This seams to be quite acceptable to me. 

Whenever, we have to read the excact same dataset more than once, the fastest is always to load the dataset the first time from the file, then save the R object in binary format and then load it again using the `load()` command. For the above dataframe `myData` this is done as follows.

```{r}
sDataBinFn <- "../data/mydata.bin"
save(myData, file = sDataBinFn)
(ptBinLoad <- system.time(load(file = sDataBinFn)))
```

Comparing the above timing to the minimal version using the `read.table()` function shows that loading a data object from a binary file is `r round(as.numeric(ptReadTable[3])/as.numeric(ptBinLoad[3]), digits = 0)` times faster than the minimal version. This makes clear that reading the exact same dataset from a file more than once, is a waste of time. 

There is a clear trade-off between the time it takes to read a certain dataset from a file and the amount of information we have to know beforehand about this dataset. Whenever, we need the data in a tabulare form like a data.frame or a matrix it takes longer to read and convert that data compared to when we just read the data into a vector of strings.

The function `readLines()` allows us to read a file line-by-line and therebye offers the possibility of processing streams of data commonly observed in big data projects.


## Section 3 - Loops and Vectorization
It is true, R is not FORTRAN and not C. That means, programming practices that are common in those two languages do normally not translate very well into R. Translation here means pure syntactic conversion of code from FORTRAN or C into R statements. The fact that the R kernel is written in C and that many compiled libraries are written in FORTRAN that are available in R does not help a lot here. 

```
To be continued ...
```


## Session Info
```{r}
sessionInfo()
```

```{r, echo=FALSE}
### # in case we do not want to keep the data, clean up
if (!bKeepData) unlink(c(sSnpPhenFn, sSnpFn, sDataBinFn))
```


## References
