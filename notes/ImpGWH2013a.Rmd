---
title: "Literate Programming Based Improvements To Chapter 1 of GWH2013a"
author: "Peter von Rohr"
date: "29 Jan 2015"
layout: post
output:
  html_document:
    keep_md: no
    theme: united
    mathjax: "http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
references:
- URL: http://www.springer.com/life+sciences/systems+biology+and+bioinformatics/book/978-1-62703-446-3
  author:
  - family: Gondro
    given: Cedric
  - family: van der Werf
    given: Julius
  - family: Hayes
    given: Ben
  id: GWH2013a
  issued:
    year: 2013
  title: Genome-Wide Association Studies and Genomic Prediction
---

## Disclaimer
This document summarizes the content of chapter 1 of [@GWH2013a]. The main objective of this summary is to transform the summarized text and the programming code between the text into a document based on the paradigm of [`Literate Programming`](http://en.wikipedia.org/wiki/Literate_programming). 

## Chapter 1 - R for Genome Wide Association Studies

### Abstract
The authors state that R has become the de facto language of choice for statisticians and that it is the most widely used environment for analysing high-throughput genomic data. While these statements are certainly not wrong, one has to keep in mind that most of statistical analyses are probably still done in MS Excel. But despite that fact I would never recommend to anyone to use MS Excel for any statistical analysis. Or more generally I would not make any recommendation of a product only based on the fact that a large number of people use that product. From my point of view, R is widely used because it has a very active and supportive community and that problems are solved much quicker than in any other commericially available products. 

### Section 1 - Introduction
The introduction starts with the same statements as were already mentioned at the begining of the abstract. Do not get me wrong here, I am not critisizing the authors here for saying anything wrong, but I find those arguments not so convincing. Part of the problem is also that no examples of R-code snippets are shown to support their argument. 

Let me add two points in favor of R that I would use to convince people to use R. For all those readers who are not interested in these arguments or for all those who are already convinced of R, can safely skip the remainder of this subsection and can jump to the summary of section 2 of chapter 1 of [@GWH2013a].

My favorite two arguments in favor of R are

1. `Fast Prototyping` - which means that ideas are quickly converted into R-code and
2. `Literate Programming` - R provides a lot of infrastructure to implement the paradigm of literate programming. 

For all those who want to know more about the above two points, please read the post entitled [Why R](http://charlotte-ngs.github.io/WhyR/notes/20150203-WhyR.html). 

After this short digression, let us get back to [@GWH2013a]


## Section 2 - Reading Data into R
It is true that it takes time to read genomic data of a certain size into R, but this is true for any system. Using the same dataset as in [@GWH2013a], the following timings result

```{r}
sSnpDataFn <- "../data/square.txt"
(ptReadTable  <- system.time(myData <- read.table(file = sSnpDataFn, sep = "\t")))
dim(myData)
```

### What about MS Excel?
As a very small side note, when looking at the dimension of the data.frame `myData`, we can see that it has `r ncol(myData)` columns. According to [Excel specifications and limits](https://support.office.microsoft.com/en-us/article/Excel-specifications-and-limits-ca36e2dc-1f09-4620-b726-67c00b05040f?CTT=1&CorrelationId=f389929d-fd39-4369-b67d-a8061edd2258&ui=en-US&rs=en-US&ad=US) the number of columns that can be read into a single worksheet in Excel is still limited to 16,384 which makes it clearly impossible to read this dataset into a single worksheet of MS Excel. I clearly prefer waiting for `r as.numeric(ptReadTable[3])` seconds until the data is read compared to not being able to read the data at all. 

Back on track to where we were before that side note, we still want to read that SNP dataset into R and we would like to do it a little faster then what we just saw with a bare-bones `read.table` command. One way of doing this is by telling the `read.table()` function what datatype to expect in which column. Datatypes are specified using the argument `colClasses` which is set to a vector of strings where each vector element contains the datatype of the corresponding datacolumn. 

```{r}
(ptReadTableColClasses <- system.time(myData <- read.table(file = sSnpDataFn, sep = "\t", 
                                                           colClasses = c("character", "factor", 
                                                                          rep("numeric",50001)))))
```

### Uups why is this not working?
When we compare the above statement to the original one in [@GWH2013a], we can see that there is a difference. The vector assigned to the `colClasses` argument in the book is missing the first element. The statement in the book when copy-pasted to the R-prompt gives the following error:

```
myData <- read.table(file = sSnpDataFn, sep = "\t", colClasses = c("factor", rep("numeric",50001)))
Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  : 
  scan() expected 'a real', got 'A'
```

The reason for the error can be found in the help file to `?read.table()` which says for the argument colClasses that row names must also be specified in the vector of column classes. Therefore when adding "character" as the first element of the colClasses vector, the error disappears. 

### Bummer - Literate Programming Rocks!
This is really unbelievable, the authors of the first chapter in [@GWH2013a] could not have given me a bigger gift with this error. I really feel blessed to be able to present such a fantastic show-case in favor of literate programming. Would the authors have used a literate programming approach, such as we are doing here, they could have avoided for themselves the embarrasing and painful experience of showing an incorrect statements already in the second code snippet that they are presenting. For the reader of the book who really wants to use the examples, this is very frustrating when you hit an error already in the second statement that could be found. 

In the text after the code snippet producing the error, there is yet another problem. The authors mention that performance could still be marginally improved by specifying the number of rows in the table beforehand using in out case `ncols=50000`. But this does not work. First, we do not have 50000 rows but only `r nrow(myData)` rows. Second, the function `read.table()` does not have an argument called `ncol`. Such an argument would make no sense at all, because the number of columns is already determined by the colClasses vector. Probably, what the authors wanted to say is to specify the number of rows using the `nrows` argument such as shown below. 

Adding more options concerning the comment character and the number of rows, we can hope to still get a little faster.

```{r}
(ptReadTableNRows <- system.time(myData <- read.table(file = sSnpDataFn, sep = "\t", 
                                                           colClasses = c("character", "factor", 
                                                                          rep("numeric",50001)),
                                                           comment.char = "",
                                                           nrows = 1000)))

```




```
To be continued ...
```

## Session Info
```{r}
sessionInfo()
```

## References
